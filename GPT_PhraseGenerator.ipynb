{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Constants\n",
    "from  openai import OpenAI\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import gspread\n",
    "import gspread_dataframe as gd\n",
    "import gspread_formatting as gf\n",
    "from gspread_formatting import cellFormat, color, textFormat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Phrase Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.phrase_generator import *\n",
    "\n",
    "prompt = get_prompt_to_gen_phrases(\n",
    "            situation = \"Going to the library\",\n",
    "            num_phrases = 5,\n",
    "            complexity = \"Medium\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model=\"gpt-4o\"\n",
    "temp=0.7 \n",
    "\n",
    "translation_response =  get_completion(\n",
    "        prompt = prompt,\n",
    "        model=translation_model, \n",
    "        temperature=temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_translation_response(\n",
    "        content: str, \n",
    "        ffill_cols: List[str] = None,\n",
    "        date_col: List[str] = None\n",
    "        ) -> pd.DataFrame:\n",
    "    '''\n",
    "    Parse the table response from OpenAI into a pandas DataFrame\n",
    "    '''\n",
    "    # Using StringIO to treat the text as a file-like object for pandas\n",
    "    data = StringIO(content)\n",
    "\n",
    "    # Read the table into a pandas DataFrame\n",
    "    df = pd.read_csv(data, delimiter='|',  engine='python')\n",
    "\n",
    "    # Cleaning the DataFrame by stripping leading/trailing whitespaces from column names and data\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df = df.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    \n",
    "    # Remove rows with dashes\n",
    "    col_name = df.select_dtypes(include=['object']).columns[0]\n",
    "    df = df.loc[~df[col_name].str.contains('--')]\n",
    "\n",
    "    col_to_keep = [col for col in df if 'Unnamed' not in col]\n",
    "    df = df[col_to_keep]\n",
    "\n",
    "    if ffill_cols:\n",
    "        for col in ffill_cols:\n",
    "            df[col] = df[col].replace('', pd.NA).ffill()\n",
    "\n",
    "    if date_col:\n",
    "        for col in date_col:\n",
    "            df[col] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_translation_response(translation_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Line', 'Pinyin', 'Meaning', 'Response', 'Response Pinyin',\n",
       "       'Response Meaning', 'Complexity', 'Category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "newwords_df = (\n",
    "    parse_translation_response(\n",
    "        translation_response.choices[0].message.content,\n",
    "        ffill_cols = ['Word', 'Pinyin', 'Pinyin Simplified', 'Type'],\n",
    "        date_col = ['Added Date']\n",
    "        )\n",
    "    )\n",
    "\n",
    "## Generate words rarity classification\n",
    "rarity_response = (\n",
    "    get_completion(\n",
    "        prompt=get_prompt_for_rarity_classification(word_list), model=rarity_model, temperature=temp))\n",
    "\n",
    "word_rarity_df = parse_translation_response(rarity_response.choices[0].message.content)\n",
    "newwords_df = pd.merge(newwords_df, word_rarity_df, on='Word', how='left')\n",
    "\n",
    "if not (replace_new_words) and len(self.new_words_df) > 0:\n",
    "    orig_new_words_df = self.new_words_df.copy()\n",
    "\n",
    "    if len(orig_new_words_df) > 0:\n",
    "        orig_new_words_df = orig_new_words_df.loc[~orig_new_words_df.Word.isin(newwords_df.Word)]\n",
    "    self.new_words_df = pd.concat([orig_new_words_df, newwords_df])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dash_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
